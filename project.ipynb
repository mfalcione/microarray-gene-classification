{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "backed-collective",
   "metadata": {
    "id": "sophisticated-sunrise"
   },
   "source": [
    "# Project\n",
    "## BMES 543\n",
    "### Matthew Falcione"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-vacuum",
   "metadata": {
    "id": "ecological-naples"
   },
   "source": [
    "### Adjust Notebook Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "magnetic-miracle",
   "metadata": {
    "id": "great-turkey"
   },
   "outputs": [],
   "source": [
    "# set constants that the user can adjust to fit their system constraints and needs\n",
    "# set LOCAL to False if running on Google Colab\n",
    "LOCAL = True\n",
    "# set SKIP_PREPROCESSING to False if you want to run all the preprocessing steps\n",
    "# *WARNING* it will take over an hour to run the feature selection steps on Google Colab\n",
    "SKIP_PREPROCESSING = True\n",
    "# set SKIP_RUN to False if you want to run all the classification steps\n",
    "# *WARNING* it will take over two hours to run the classification steps on Google Colab\n",
    "SKIP_RUN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-allen",
   "metadata": {
    "id": "ambient-toronto"
   },
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "moved-mechanism",
   "metadata": {
    "id": "loved-enzyme"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import zipfile\n",
    "import warnings\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, chi2, RFECV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "broadband-bryan",
   "metadata": {
    "id": "copyrighted-dominant"
   },
   "outputs": [],
   "source": [
    "# adjust notebook display settings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = 50\n",
    "\n",
    "# add BMES directory if on local machine\n",
    "if LOCAL:\n",
    "    sys.path.append(os.environ['BMESAHMETDIR'])\n",
    "    import bmes\n",
    "else:\n",
    "    # Google Colab requires using joblib to load pickle files instead of pickle.load\n",
    "    from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-cambridge",
   "metadata": {
    "id": "potential-tyler"
   },
   "source": [
    "### Download dataset from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "decent-firmware",
   "metadata": {
    "id": "sealed-universe"
   },
   "outputs": [],
   "source": [
    "# create dictionaries to house all dataset data\n",
    "# these will be nested dictonaries that contain all relevant data for each respective dataset\n",
    "dataset_name = ['adenocarcinoma', 'colon', 'prostate', 'lymphoma',\n",
    "                'nci', 'srbct', 'leukemia', 'brain', 'breast.2.class', 'breast.3.class']\n",
    "# initalize dictionaries for each dataset\n",
    "dataset_dict = {dataset: {} for dataset in dataset_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "experimental-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data and class file paths\n",
    "def get_data_class_file_paths(dataset_name):\n",
    "    if LOCAL:\n",
    "        data_file_path = f'{bmes.tempdir()}/data.sets/{dataset_name}.data.txt'\n",
    "        class_file_path = f'{bmes.tempdir()}/data.sets/{dataset_name}.class.txt'\n",
    "    else:\n",
    "        data_file_path = f'data.sets/{dataset_name}.data.txt'\n",
    "        class_file_path = f'data.sets/{dataset_name}.class.txt'\n",
    "        \n",
    "    return data_file_path, class_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sitting-ending",
   "metadata": {
    "id": "threaded-youth"
   },
   "outputs": [],
   "source": [
    "# retrieve file from url\n",
    "def download_files(dataset_dict):\n",
    "    \n",
    "    # test if the data files are present\n",
    "    try:\n",
    "        # loop through each dataset's dict for file names\n",
    "        for dataset_name in dataset_dict:\n",
    "            # generate file paths for the corresponding data and class label files\n",
    "            data_file_path, class_file_path = get_data_class_file_paths(dataset_name)\n",
    "\n",
    "            # ensure that the file paths exist\n",
    "            assert (os.path.exists(data_file_path) and os.path.exists(class_file_path)), 'Files are not in the data directory.'       \n",
    "        \n",
    "            # add file paths to respective datasets\n",
    "            dataset_dict[dataset_name]['data_file_path'] = data_file_path\n",
    "            dataset_dict[dataset_name]['class_file_path'] = class_file_path\n",
    "            \n",
    "        print('Dataset files already downloaded, skipping...')\n",
    "        \n",
    "        return dataset_dict\n",
    "                \n",
    "    except AssertionError as e: \n",
    "        print(e)\n",
    "        print('Downloading datasets.')\n",
    "        \n",
    "        # dataset value and class download link\n",
    "        GITHUB_DATASET_LINK = 'https://github.com/rdiaz02/varSelRF-suppl-mat/blob/3ef2b5156f288cdad153998084bf77578c90393d/data.sets.zip?raw=true'\n",
    "        \n",
    "        # generate file paths for the zipped and unzipped data and class label files\n",
    "        if LOCAL:\n",
    "            github_zipped_folder_path = f'{bmes.tempdir()}/data.sets.zip'\n",
    "            github_unzipped_folder_path = bmes.tempdir()\n",
    "        else:\n",
    "            github_zipped_folder_path = f'data.sets.zip'\n",
    "            github_unzipped_folder_path = \"\"\n",
    "            \n",
    "        if not os.path.exists(github_zipped_folder_path):\n",
    "            # download the dataset from GitHub\n",
    "            dataset_request = requests.get(GITHUB_DATASET_LINK, allow_redirects=True)\n",
    "            # write folder to github_zipped_folder_path\n",
    "            with open(github_zipped_folder_path, 'wb') as file:\n",
    "                file.write(dataset_request.content)\n",
    "            \n",
    "        # extract datasets folder\n",
    "        with zipfile.ZipFile(github_zipped_folder_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(github_unzipped_folder_path)\n",
    "        \n",
    "        \n",
    "        # loop through each dataset's dict for file names\n",
    "        for dataset_name in dataset_dict:\n",
    "            # generate file paths for the corresponding data and class label files\n",
    "            data_file_path, class_file_path = get_data_class_file_paths(dataset_name)\n",
    "\n",
    "            # add file paths to respective datasets\n",
    "            dataset_dict[dataset_name]['data_file_path'] = data_file_path\n",
    "            dataset_dict[dataset_name]['class_file_path'] = class_file_path\n",
    "\n",
    "    print('Done!')\n",
    "        \n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "precious-serial",
   "metadata": {
    "id": "personal-verification"
   },
   "outputs": [],
   "source": [
    "# download dataset files and class labels if running the data preprocesing steps\n",
    "if not SKIP_PREPROCESSING:\n",
    "    dataset_dict = download_files(dataset_dict=dataset_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-theater",
   "metadata": {
    "id": "signal-serbia"
   },
   "source": [
    "### Load and clean datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "committed-geology",
   "metadata": {
    "id": "desirable-politics"
   },
   "outputs": [],
   "source": [
    "# convert text class labels to numeric (this is present in some of the datasets)\n",
    "def clean_class_data(class_array):\n",
    "    # covert text labels of 'Normal'/'Tumor' to numeric\n",
    "    if 'Normal' in list(class_array):\n",
    "        return np.where(class_array == 'Normal', 0, 1)\n",
    "    else:\n",
    "        return class_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "upper-helmet",
   "metadata": {
    "id": "roman-fifty"
   },
   "outputs": [],
   "source": [
    "# create dictionaries of the downloaded file's class target values, attribute values, and attribute names\n",
    "def load_datasets(dataset_dict):\n",
    "    # X: attribute values\n",
    "    # y: target values\n",
    "    # names: attribute names\n",
    "\n",
    "    # loop through each dataset\n",
    "    for dataset_name in dataset_dict:\n",
    "        # get file paths for data and class files\n",
    "        data_file_path = dataset_dict[dataset_name]['data_file_path']\n",
    "        class_file_path = dataset_dict[dataset_name]['class_file_path']\n",
    "        \n",
    "        df_data = pd.read_csv(data_file_path, delimiter='\\t')\n",
    "        # determine if the gene column was set as index\n",
    "        # certain datasets used '#' to denote the index column\n",
    "        if df_data.columns[0] == '#':\n",
    "            df_data = pd.read_csv(data_file_path, delimiter='\\t', index_col='#')\n",
    "        df_class = pd.read_csv(class_file_path, delimiter='\\t', header=None)\n",
    "\n",
    "        # drop missing samples (columns)\n",
    "        df_class.dropna(inplace=True, axis='columns')\n",
    "        df_data.dropna(inplace=True, axis='columns')\n",
    "\n",
    "        # append each dataset's class target values, attribute values, and attribute names to their respective dictionaries\n",
    "        dataset_dict[dataset_name]['X'] = df_data.T.values\n",
    "        dataset_dict[dataset_name]['y'] = clean_class_data(df_class.values[0])\n",
    "        dataset_dict[dataset_name]['names'] = list(df_data.T.columns)\n",
    "        \n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "suburban-shift",
   "metadata": {
    "id": "olympic-dodge"
   },
   "outputs": [],
   "source": [
    "# load dataset class target values, attribute values, and attribute names if running the data preprocesing steps\n",
    "if not SKIP_PREPROCESSING:\n",
    "    dataset_dict = load_datasets(dataset_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-chemical",
   "metadata": {
    "id": "closing-violation"
   },
   "source": [
    "### Create repeated stratified k-fold cross-validation generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "julian-validity",
   "metadata": {
    "id": "uniform-scout"
   },
   "outputs": [],
   "source": [
    "# 3-fold, 10 repeat cross-validation\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=10, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-warehouse",
   "metadata": {
    "id": "liked-sleeping"
   },
   "source": [
    "### Initialize classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "peaceful-tackle",
   "metadata": {
    "id": "severe-tablet"
   },
   "outputs": [],
   "source": [
    "# initiate random forest classifier model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "# initiate XGBoost classifier model\n",
    "xgboost_model = XGBClassifier(random_state=0)\n",
    "\n",
    "# control how many hidden units are used for the network\n",
    "NUM_HIDDEN = 5\n",
    "# initiate neural network classifier model\n",
    "nn_model = MLPClassifier(solver='adam', hidden_layer_sizes=(NUM_HIDDEN,), random_state=0, max_iter=1000)\n",
    "\n",
    "# initiate LDA model\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "\n",
    "# initiate KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# initiate logisitc regression model\n",
    "lr_model = LogisticRegression(random_state=0)\n",
    "\n",
    "# initiate SVM model\n",
    "svm_model = SVC(kernel='rbf', probability=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-drain",
   "metadata": {
    "id": "built-biology"
   },
   "source": [
    "### Create feature selection methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-nursery",
   "metadata": {
    "id": "chief-vehicle"
   },
   "source": [
    "* K-best feature selection (filter method)\n",
    "* Tree-based feature selection (embedded method)\n",
    "* Recursive feature elimination (hybrid method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "olive-retro",
   "metadata": {
    "id": "western-ghost"
   },
   "outputs": [],
   "source": [
    "# create dictionary to house feature selection types and corresponding X dataset label\n",
    "feature_selection_types = ['Raw', 'K-best', 'Tree-based', 'Random Forest RFE', 'XGBoost RFE', 'Logistic Regression RFE']\n",
    "X_dataset_labels = ['X', 'X_k_best', 'X_tree_based', 'X_random_forest_rfe', 'X_xgboost_rfe', 'X_lr_rfe']\n",
    "feature_selection_dict = dict(zip(feature_selection_types, X_dataset_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-equality",
   "metadata": {
    "id": "unexpected-advance"
   },
   "source": [
    "#### K-best (univariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "thermal-soldier",
   "metadata": {
    "id": "literary-pharmacology"
   },
   "outputs": [],
   "source": [
    "# if running the data preprocesing steps\n",
    "if not SKIP_PREPROCESSING:\n",
    "    # apply SelectKBest class to extract top 10 best features using a chi^2 distribution\n",
    "    k = 10\n",
    "    best_features = SelectKBest(score_func=chi2, k=k)\n",
    "\n",
    "    # fit the feature selction to the X and y values\n",
    "    for dataset_name in dataset_dict:\n",
    "        # add dataset's selected feature subset to dict\n",
    "        # X values must be positive\n",
    "        dataset_dict[dataset_name]['X_k_best'] = best_features.fit_transform(abs(dataset_dict[dataset_name]['X']),\n",
    "                                                                             dataset_dict[dataset_name]['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-patient",
   "metadata": {
    "id": "round-controversy"
   },
   "source": [
    "#### Tree-based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "affected-inclusion",
   "metadata": {
    "id": "indoor-renewal"
   },
   "outputs": [],
   "source": [
    "# if running the data preprocesing steps\n",
    "if not SKIP_PREPROCESSING:\n",
    "    for dataset_name in dataset_dict:\n",
    "        # fit the random forest model to X and y\n",
    "        _ = rf_model.fit(dataset_dict[dataset_name]['X'], dataset_dict[dataset_name]['y'])\n",
    "        # get the feature importances\n",
    "        importances = rf_model.feature_importances_\n",
    "\n",
    "        # get the top 10 most important attributes from X\n",
    "        names = dataset_dict[dataset_name]['names']\n",
    "        importances_series = pd.Series(importances, index=names)\n",
    "        top_n_most_important_attributes = importances_series.nlargest(10).index.tolist()\n",
    "\n",
    "        # get the column numbers of X from the top 10 most important features\n",
    "        attribute_column_numbers = [names.index(attribute) for attribute in top_n_most_important_attributes]\n",
    "\n",
    "        # extract 10 most important features from X\n",
    "        dataset_dict[dataset_name]['X_tree_based']  = dataset_dict[dataset_name]['X'][:, attribute_column_numbers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-norwegian",
   "metadata": {
    "id": "bored-journalist"
   },
   "source": [
    "#### Recursive feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "offensive-samba",
   "metadata": {
    "id": "engaged-conspiracy"
   },
   "outputs": [],
   "source": [
    "# if running the data preprocesing steps\n",
    "if not SKIP_PREPROCESSING:\n",
    "    for dataset_name in dataset_dict:\n",
    "        # build the RFE with cross-validation on the random-forest model (remove 10% in each iteration -> step=0.1)\n",
    "        random_forest_rfe = RFECV(rf_model, min_features_to_select=1, step=0.1, cv=cv, scoring='accuracy')\n",
    "        xgboost_rfe = RFECV(xgboost_model, min_features_to_select=1, step=0.1, cv=cv, scoring='accuracy')\n",
    "        lr_rfe = RFECV(lr_model, min_features_to_select=1, step=0.1, cv=cv, scoring='accuracy')\n",
    "\n",
    "        # fit RFE to data\n",
    "        rf_selector = random_forest_rfe.fit(dataset_dict[dataset_name]['X'], dataset_dict[dataset_name]['y'])\n",
    "        xgboost_selector = xgboost_rfe.fit(dataset_dict[dataset_name]['X'], dataset_dict[dataset_name]['y'])\n",
    "        lr_selector = lr_rfe.fit(dataset_dict[dataset_name]['X'], dataset_dict[dataset_name]['y'])\n",
    "\n",
    "        # get boolean support values for each variable (gene)\n",
    "        rf_selected_features_boolean_index = rf_selector.support_\n",
    "        xgboost_selected_features_boolean_index = xgboost_selector.support_\n",
    "        lr_selected_features_boolean_index = lr_selector.support_\n",
    "\n",
    "        # select index of features that were kept by feature selection model\n",
    "        rf_selected_features_index = rf_selected_features_boolean_index.nonzero()[0]\n",
    "        xgboost_selected_features_index = xgboost_selected_features_boolean_index.nonzero()[0]\n",
    "        lr_selected_features_index = lr_selected_features_boolean_index.nonzero()[0]\n",
    "\n",
    "        # extract kept features from data\n",
    "        dataset_dict[dataset_name]['X_random_forest_rfe']  = dataset_dict[dataset_name]['X'][:, rf_selected_features_index]\n",
    "        dataset_dict[dataset_name]['X_xgboost_rfe']  = dataset_dict[dataset_name]['X'][:, xgboost_selected_features_index]\n",
    "        dataset_dict[dataset_name]['X_lr_rfe']  = dataset_dict[dataset_name]['X'][:, lr_selected_features_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-uncertainty",
   "metadata": {
    "id": "genuine-detroit"
   },
   "source": [
    "### Save Dataset with Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aboriginal-spencer",
   "metadata": {
    "id": "systematic-terrorist"
   },
   "outputs": [],
   "source": [
    "# data_file_path will use the lcoal directory if on Google Colab\n",
    "# LOCAL will use bmes.tempdir()\n",
    "if not LOCAL:\n",
    "    data_file_path = 'dataset_dict.pickle'\n",
    "else:\n",
    "    data_file_path = f'{bmes.tempdir()}/dataset_dict.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "exotic-terrorist",
   "metadata": {
    "id": "sexual-tissue"
   },
   "outputs": [],
   "source": [
    "# if running the data preprocesing steps\n",
    "if not SKIP_PREPROCESSING:\n",
    "    # save dictonaries with data so we don't have to run the code again\n",
    "    pickle.dump(dataset_dict, open(data_file_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-aging",
   "metadata": {
    "id": "accurate-presentation"
   },
   "source": [
    "### Load Selected Features Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "intermediate-swedish",
   "metadata": {
    "id": "august-perspective"
   },
   "outputs": [],
   "source": [
    "# if preprocessed data file does not exist\n",
    "if not os.path.exists(data_file_path):\n",
    "    # if not running the data preprocesing steps\n",
    "    if SKIP_PREPROCESSING: \n",
    "        # initiate dataset_dict link\n",
    "        dataset_dict_link = 'https://drexel0-my.sharepoint.com/:u:/g/personal/mjf378_drexel_edu/ER9Yrk5KV6xJvYqvCO975TcBFESzfq8fElv346c9-dD_GQ?download=1'\n",
    "        dataset_dict_request = requests.get(dataset_dict_link, allow_redirects=True)\n",
    "\n",
    "        # write files to bmes.datadir() or local directory with respective names\n",
    "        with open(data_file_path, 'wb') as file:\n",
    "            file.write(dataset_dict_request.content)\n",
    "\n",
    "# load dataset_dict from pickle file\n",
    "if LOCAL:\n",
    "    dataset_dict = pickle.load(open(data_file_path, 'rb'))\n",
    "else:\n",
    "    # Google Colab requires using joblib to load pickle files instead of pickle.load\n",
    "    dataset_dict = joblib.load(data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-column",
   "metadata": {
    "id": "italian-issue"
   },
   "source": [
    "### Create classification evaluation metric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "rough-favorite",
   "metadata": {
    "id": "abandoned-alexandria"
   },
   "outputs": [],
   "source": [
    "def classification_evaluation_metrics(model, X, y, cv, classification_type=None, feature_selection_type=None, dataset_name=None):\n",
    "    # calculate accuracy value from model\n",
    "    # in multi-class case (where only one possible target) - micro-precision=micro-recall=micro-F1=accuracy\n",
    "    # micro: calculate metrics globally by considering each element of the label indicator matrix as a label\n",
    "    accuracy_scores = cross_val_score(model, X, y, scoring='f1_micro', cv=cv, error_score='raise')\n",
    "    accuracy = round(np.median(accuracy_scores), 3)\n",
    "    \n",
    "    # calculate AUC-ROC value from model\n",
    "    # ovo: stands for one-vs-one, computes the average AUC of all possible pairwise combinations of classes\n",
    "    auc_roc_scores = cross_val_score(model, X, y, scoring='roc_auc_ovo_weighted', cv=cv, error_score='raise')\n",
    "    auc_roc = round(np.median(auc_roc_scores), 3)\n",
    "\n",
    "    return accuracy, auc_roc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-teaching",
   "metadata": {
    "id": "circular-morris"
   },
   "source": [
    "### Run classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "vocal-avatar",
   "metadata": {
    "id": "sitting-murder"
   },
   "outputs": [],
   "source": [
    "# create list of columns for the model output dataframe\n",
    "column_list = ['Dataset Name','Classification Type'] + feature_selection_types\n",
    "# initialize dict of classification models and classification type descriptions\n",
    "model_list = [rf_model, xgboost_model, nn_model, lda_model, knn_model, lr_model, svm_model]\n",
    "classification_type_list = ['Random Forest', 'XGBoost', 'Neural Network', 'LDA', 'KNN', ' Logistic Regression', 'SVM']\n",
    "model_classification_dict = dict(zip(model_list, classification_type_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "parallel-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_file_path(suffix):\n",
    "    # output_file_path will use the local directory if on Google Colab\n",
    "    # LOCAL will use bmes.tempdir()\n",
    "    if not LOCAL:\n",
    "        accuracy_output_file_path = f'accuracy_list_{suffix}.pickle'\n",
    "        auc_roc_output_file_path = f'auc_roc_list_{suffix}.pickle'\n",
    "    else:\n",
    "        accuracy_output_file_path = f'{bmes.tempdir()}/accuracy_list_{suffix}.pickle'\n",
    "        auc_roc_output_file_path = f'{bmes.tempdir()}/auc_roc_list_{suffix}.pickle'\n",
    "        \n",
    "    return accuracy_output_file_path, auc_roc_output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "colonial-hypothesis",
   "metadata": {
    "id": "foster-rental"
   },
   "outputs": [],
   "source": [
    "def export_results(accuracy_list, auc_roc_list, suffix):\n",
    "    # get file path for output files\n",
    "    accuracy_output_file_path, auc_roc_output_file_path = get_output_file_path(suffix)\n",
    "        \n",
    "    # save lists with the output so we don't have to run the code again\n",
    "    pickle.dump(accuracy_list, open(accuracy_output_file_path, \"wb\"))\n",
    "    pickle.dump(auc_roc_list, open(auc_roc_output_file_path, \"wb\"))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "future-acoustic",
   "metadata": {
    "id": "technical-germany"
   },
   "outputs": [],
   "source": [
    "def run_classification_models(cv, column_list, dataset_dict, feature_selection_dict, model_classification_dict):\n",
    "    # create lists to contain the model results for each dataset and feature selection method\n",
    "    # lists will contain dicts which will each be a row in the final corresponding dataframe\n",
    "    accuracy_list = []\n",
    "    auc_roc_list = []\n",
    "    \n",
    "    # loop thorough each classification model\n",
    "    for model, classification_type in model_classification_dict.items():\n",
    "        print(classification_type)\n",
    "        print('-------------------')\n",
    "         \n",
    "        # loop through each dataset\n",
    "        for dataset_name in dataset_dict:\n",
    "            # accuracy\n",
    "            # create dict that will contain model accuracy for each feature selection type\n",
    "            model_accuracy_output_dict = {col: None for col in column_list}\n",
    "            # add dataset name and classification type to dict\n",
    "            model_accuracy_output_dict['Dataset Name'] = dataset_name\n",
    "            model_accuracy_output_dict['Classification Type'] = classification_type\n",
    "\n",
    "            # AUC-ROC\n",
    "            # create dict that will contain model AUC-ROC for each feature selection type\n",
    "            model_auc_roc_output_dict = {col: None for col in column_list}\n",
    "            # add dataset name and classification type to dict\n",
    "            model_auc_roc_output_dict['Dataset Name'] = dataset_name\n",
    "            model_auc_roc_output_dict['Classification Type'] = classification_type\n",
    "            \n",
    "            # run classification model with each feature selection technique and dataset\n",
    "            for feature_selection_type, X_dataset_label in feature_selection_dict.items(): \n",
    "                print(f'{dataset_name} - {feature_selection_type}')\n",
    "\n",
    "                # output accauracy and aur-roc for the combinations of each model and feature selection technique\n",
    "                accuracy, auc_roc = classification_evaluation_metrics(model=model, X=dataset_dict[dataset_name][X_dataset_label],\n",
    "                                                                      y=dataset_dict[dataset_name]['y'], cv=cv,\n",
    "                                                                      classification_type=classification_type,\n",
    "                                                                      feature_selection_type=feature_selection_type,\n",
    "                                                                      dataset_name=dataset_name)\n",
    "            \n",
    "                # add each feature selection technique output to output dict for the respective dataset\n",
    "                model_accuracy_output_dict[feature_selection_type] = accuracy\n",
    "                model_auc_roc_output_dict[feature_selection_type] = auc_roc\n",
    "                \n",
    "            # append model output dict for specific dataset to list of outputs\n",
    "            accuracy_list.append(model_accuracy_output_dict)\n",
    "            auc_roc_list.append(model_auc_roc_output_dict)\n",
    "            \n",
    "        # export results after each model in case there are any issues with the time to run the models\n",
    "        export_results(accuracy_list, auc_roc_list, suffix='intermediate_output')\n",
    "        print('')\n",
    "    \n",
    "    # save final output - this is here so that if run_classification_models is run it will not overwrite the finalized output with the intermediate output \n",
    "    export_results(accuracy_list, auc_roc_list, suffix='final_output')        \n",
    "    \n",
    "    return accuracy_list, auc_roc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "unlike-airfare",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YeC_jicgjgXl",
    "outputId": "95ae06c1-d990-41d2-ffa4-61da1e6b2334"
   },
   "outputs": [],
   "source": [
    "# run all classification models\n",
    "if not SKIP_RUN:\n",
    "    accuracy_list, auc_roc_list = run_classification_models(cv=cv, column_list=column_list, dataset_dict=dataset_dict,\n",
    "                                                          feature_selection_dict=feature_selection_dict, model_classification_dict=model_classification_dict)\n",
    "else:\n",
    "    # get file path for output files\n",
    "    accuracy_output_file_path, auc_roc_output_file_path = get_output_file_path(suffix='final_output')\n",
    "    # load dataset_dict from pickle file\n",
    "    if LOCAL:\n",
    "        accuracy_list = pickle.load(open(accuracy_output_file_path, 'rb'))\n",
    "        auc_roc_list = pickle.load(open(auc_roc_output_file_path, 'rb'))\n",
    "    else:\n",
    "        # Google Colab requires using joblib to load pickle files instead of pickle.load\n",
    "        accuracy_list = joblib.load(accuracy_output_file_path)\n",
    "        auc_roc_list = joblib.load(auc_roc_output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "changing-accreditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main index is the dataset names\n",
    "# second level index is the classification type\n",
    "# columns are the feature selection methods\n",
    "\n",
    "# values in the columns are the accuracy values\n",
    "df_accuracy = pd.DataFrame(accuracy_list)\n",
    "df_accuracy.set_index(['Dataset Name', 'Classification Type'], inplace=True, drop=True)\n",
    "# get error rates (1 - accuracy) to directly compare to paper\n",
    "df_error_rates = (1 - df_accuracy) * 100\n",
    "\n",
    "# values in the columns are the auc-roc values\n",
    "df_auc_roc = pd.DataFrame(auc_roc_list)\n",
    "df_auc_roc.set_index(['Dataset Name', 'Classification Type'], inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-affiliate",
   "metadata": {},
   "source": [
    "### Displaying Results\n",
    "Since only accuracy/error rates were provided by the paper, they are the only metric that will be analyzed here. Further work could be done to compare AUC-ROC score outputs along with supplementing the datasets with simulated data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-debut",
   "metadata": {},
   "source": [
    "#### Pull results from paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "silent-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"no info\" denotes the minimal error we can make if we use no information from the genes\n",
    "# (i.e., we always bet on the most frequent class)\n",
    "no_info_error_rates_dict = {'adenocarcinoma': 0.158, 'colon': 0.355, 'prostate': 0.490,\n",
    "                            'lymphoma': 0.323, 'nci': 0.852, 'srbct': 0.635, 'leukemia': 0.289,\n",
    "                            'brain': 0.762, 'breast.2.class': 0.429, 'breast.3.class': 0.537}\n",
    "\n",
    "# dict of which classification method performed best for each dataset\n",
    "best_classification_method_dict = {'adenocarcinoma': 'Random Forest', 'colon': 'Srunken Centroids', 'prostate': 'Random Forest',\n",
    "                                   'lymphoma': 'Random Forest', 'nci': 'Nearest Neighbors', 'srbct': 'DLDA', 'leukemia': 'SVM',\n",
    "                                   'brain': 'SVM', 'breast.2.class': 'Srunken Centroids', 'breast.3.class': 'Random Forest'}\n",
    "\n",
    "# dict of the error rate for the classification that performed best for each dataset\n",
    "best_method_error_rates_dict = {'adenocarcinoma': 0.125, 'colon': 0.122, 'prostate': 0.061,\n",
    "                                'lymphoma': 0.009, 'nci': 0.237, 'srbct': 0.011, 'leukemia': 0.014,\n",
    "                                'brain': 0.138, 'breast.2.class': 0.324, 'breast.3.class': 0.346}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-upgrade",
   "metadata": {},
   "source": [
    "#### Calculate average performance of feature selection techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "solar-morris",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error rates of feature selection techniques\n",
      "---------------------------------------------------\n",
      "Raw: 21.1% +/- 14.995%\n",
      "K-best: 19.666% +/- 15.23%\n",
      "Tree-based: 17.514% +/- 12.51%\n",
      "Random Forest RFE: 20.456% +/- 15.26%\n",
      "XGBoost RFE: 17.557% +/- 14.461%\n",
      "Logistic Regression RFE: 15.623% +/- 15.517%\n"
     ]
    }
   ],
   "source": [
    "# calculate mean and std of performance of each feature selection technique across and datasets and models\n",
    "feature_selection_error_rates_mean = round(df_error_rates.mean(), 3)\n",
    "feature_selection_error_rates_std = round(df_error_rates.std(), 3)\n",
    "\n",
    "# display the average performance of feature selection techniques\n",
    "formatted_average_output_list = []\n",
    "print('Average error rates of feature selection techniques')\n",
    "print('---------------------------------------------------')\n",
    "for feature_selection_type in feature_selection_types:\n",
    "    formatted_average_output = f'{feature_selection_error_rates_mean[feature_selection_type]}% +/- {feature_selection_error_rates_std[feature_selection_type]}%'\n",
    "    formatted_average_output_list.append(formatted_average_output)\n",
    "    print(f'{feature_selection_type}: {formatted_average_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-brother",
   "metadata": {},
   "source": [
    "**Figure 1.** Average error rates of feature selection techniques. Tree-based and Logistic Regression recursive feature elimination methods performed best with percent errors of 17.514% +/- 12.51% and 15.623% +/- 15.517%, respectively, but the standard deviations are substantial compared to the averages for all methods since this mertric was calculated across multiple datasets and models.Â "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "verified-future",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Raw</th>\n",
       "      <th>K-best</th>\n",
       "      <th>Tree-based</th>\n",
       "      <th>Random Forest RFE</th>\n",
       "      <th>XGBoost RFE</th>\n",
       "      <th>Logistic Regression RFE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adenocarcinoma</th>\n",
       "      <td>15.386 +/- 1.497</td>\n",
       "      <td>17.071 +/- 4.834</td>\n",
       "      <td>14.343 +/- 6.05</td>\n",
       "      <td>15.914 +/- 0.146</td>\n",
       "      <td>23.343 +/- 21.506</td>\n",
       "      <td>15.014 +/- 1.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colon</th>\n",
       "      <td>15.686 +/- 2.267</td>\n",
       "      <td>12.371 +/- 2.048</td>\n",
       "      <td>11.314 +/- 2.238</td>\n",
       "      <td>17.114 +/- 5.348</td>\n",
       "      <td>11.729 +/- 2.048</td>\n",
       "      <td>9.014 +/- 3.294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prostate</th>\n",
       "      <td>12.386 +/- 2.929</td>\n",
       "      <td>9.229 +/- 1.134</td>\n",
       "      <td>7.143 +/- 1.55</td>\n",
       "      <td>8.814 +/- 1.703</td>\n",
       "      <td>10.7 +/- 3.85</td>\n",
       "      <td>4.814 +/- 2.214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lymphoma</th>\n",
       "      <td>3.457 +/- 5.41</td>\n",
       "      <td>5.157 +/- 2.12</td>\n",
       "      <td>7.243 +/- 2.267</td>\n",
       "      <td>2.143 +/- 3.934</td>\n",
       "      <td>0.686 +/- 1.814</td>\n",
       "      <td>1.429 +/- 3.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nci</th>\n",
       "      <td>39.0 +/- 10.664</td>\n",
       "      <td>48.8 +/- 5.913</td>\n",
       "      <td>41.357 +/- 8.089</td>\n",
       "      <td>39.0 +/- 10.664</td>\n",
       "      <td>31.786 +/- 7.796</td>\n",
       "      <td>39.0 +/- 10.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>srbct</th>\n",
       "      <td>6.814 +/- 5.571</td>\n",
       "      <td>6.143 +/- 2.293</td>\n",
       "      <td>13.957 +/- 0.907</td>\n",
       "      <td>5.8 +/- 5.813</td>\n",
       "      <td>1.371 +/- 2.342</td>\n",
       "      <td>1.357 +/- 3.591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leukemia</th>\n",
       "      <td>7.971 +/- 6.567</td>\n",
       "      <td>7.786 +/- 4.451</td>\n",
       "      <td>1.1 +/- 2.91</td>\n",
       "      <td>7.971 +/- 6.567</td>\n",
       "      <td>6.6 +/- 5.314</td>\n",
       "      <td>2.2 +/- 5.821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brain</th>\n",
       "      <td>31.643 +/- 9.768</td>\n",
       "      <td>19.886 +/- 4.036</td>\n",
       "      <td>22.957 +/- 7.095</td>\n",
       "      <td>30.1 +/- 9.186</td>\n",
       "      <td>26.029 +/- 9.827</td>\n",
       "      <td>13.257 +/- 9.622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breast_2_class</th>\n",
       "      <td>37.429 +/- 1.782</td>\n",
       "      <td>26.657 +/- 3.3</td>\n",
       "      <td>23.686 +/- 0.865</td>\n",
       "      <td>36.371 +/- 4.235</td>\n",
       "      <td>30.486 +/- 3.051</td>\n",
       "      <td>29.457 +/- 3.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breast_3_class</th>\n",
       "      <td>41.229 +/- 1.928</td>\n",
       "      <td>43.557 +/- 3.611</td>\n",
       "      <td>32.043 +/- 1.441</td>\n",
       "      <td>41.329 +/- 2.341</td>\n",
       "      <td>32.843 +/- 4.561</td>\n",
       "      <td>40.686 +/- 1.897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>21.1% +/- 14.995%</td>\n",
       "      <td>19.666% +/- 15.23%</td>\n",
       "      <td>17.514% +/- 12.51%</td>\n",
       "      <td>20.456% +/- 15.26%</td>\n",
       "      <td>17.557% +/- 14.461%</td>\n",
       "      <td>15.623% +/- 15.517%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Raw              K-best          Tree-based  \\\n",
       "adenocarcinoma   15.386 +/- 1.497    17.071 +/- 4.834     14.343 +/- 6.05   \n",
       "colon            15.686 +/- 2.267    12.371 +/- 2.048    11.314 +/- 2.238   \n",
       "prostate         12.386 +/- 2.929     9.229 +/- 1.134      7.143 +/- 1.55   \n",
       "lymphoma           3.457 +/- 5.41      5.157 +/- 2.12     7.243 +/- 2.267   \n",
       "nci               39.0 +/- 10.664      48.8 +/- 5.913    41.357 +/- 8.089   \n",
       "srbct             6.814 +/- 5.571     6.143 +/- 2.293    13.957 +/- 0.907   \n",
       "leukemia          7.971 +/- 6.567     7.786 +/- 4.451        1.1 +/- 2.91   \n",
       "brain            31.643 +/- 9.768    19.886 +/- 4.036    22.957 +/- 7.095   \n",
       "breast_2_class   37.429 +/- 1.782      26.657 +/- 3.3    23.686 +/- 0.865   \n",
       "breast_3_class   41.229 +/- 1.928    43.557 +/- 3.611    32.043 +/- 1.441   \n",
       "Average         21.1% +/- 14.995%  19.666% +/- 15.23%  17.514% +/- 12.51%   \n",
       "\n",
       "                 Random Forest RFE          XGBoost RFE  \\\n",
       "adenocarcinoma    15.914 +/- 0.146    23.343 +/- 21.506   \n",
       "colon             17.114 +/- 5.348     11.729 +/- 2.048   \n",
       "prostate           8.814 +/- 1.703        10.7 +/- 3.85   \n",
       "lymphoma           2.143 +/- 3.934      0.686 +/- 1.814   \n",
       "nci                39.0 +/- 10.664     31.786 +/- 7.796   \n",
       "srbct                5.8 +/- 5.813      1.371 +/- 2.342   \n",
       "leukemia           7.971 +/- 6.567        6.6 +/- 5.314   \n",
       "brain               30.1 +/- 9.186     26.029 +/- 9.827   \n",
       "breast_2_class    36.371 +/- 4.235     30.486 +/- 3.051   \n",
       "breast_3_class    41.329 +/- 2.341     32.843 +/- 4.561   \n",
       "Average         20.456% +/- 15.26%  17.557% +/- 14.461%   \n",
       "\n",
       "               Logistic Regression RFE  \n",
       "adenocarcinoma        15.014 +/- 1.573  \n",
       "colon                  9.014 +/- 3.294  \n",
       "prostate               4.814 +/- 2.214  \n",
       "lymphoma                1.429 +/- 3.78  \n",
       "nci                    39.0 +/- 10.664  \n",
       "srbct                  1.357 +/- 3.591  \n",
       "leukemia                 2.2 +/- 5.821  \n",
       "brain                 13.257 +/- 9.622  \n",
       "breast_2_class        29.457 +/- 3.033  \n",
       "breast_3_class        40.686 +/- 1.897  \n",
       "Average            15.623% +/- 15.517%  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate average performance of each feature selection technique for each dataset (across all models)\n",
    "feature_selection_performance_dict = {}\n",
    "for dataset_name in dataset_dict:\n",
    "    df_feature_selection_mean = round(df_error_rates.loc[dataset_name].mean(), 3)\n",
    "    df_feature_selection_std = round(df_error_rates.loc[dataset_name].std(), 3)\n",
    "    \n",
    "    # create list of formatted mean std output\n",
    "    formatted_feature_mean_std = [f'{df_feature_selection_mean[feature]} +/- {df_feature_selection_std[feature]}' for feature in df_feature_selection_mean.index]\n",
    "    \n",
    "    # store each mean and std in a dictonary to convert into a dataframe\n",
    "    feature_selection_performance_dict[dataset_name] = formatted_feature_mean_std\n",
    "    \n",
    "# display dataframe of average performance of each feature selection technique for each dataset (across all models)\n",
    "df_features_mean_std = pd.DataFrame(feature_selection_performance_dict).T\n",
    "df_features_mean_std.columns = feature_selection_types\n",
    "# add average error rates for each feature selection technique\n",
    "df_features_mean_std.loc['Average'] = formatted_average_output_list\n",
    "df_features_mean_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-morgan",
   "metadata": {},
   "source": [
    "**Table 1:** Average performance (% error) of each feature selection technique for each dataset (across all models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "verbal-benjamin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVM - Raw</th>\n",
       "      <th>SVM - Feature Selection</th>\n",
       "      <th>SVM - Performance Increase</th>\n",
       "      <th>KNN - Raw</th>\n",
       "      <th>KNN - Feature Selection</th>\n",
       "      <th>KNN - Performance Increase</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adenocarcinoma</th>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colon</th>\n",
       "      <td>14.3</td>\n",
       "      <td>9.5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prostate</th>\n",
       "      <td>14.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>10.3</td>\n",
       "      <td>14.7</td>\n",
       "      <td>5.9</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lymphoma</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nci</th>\n",
       "      <td>35.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>33.3</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>srbct</th>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leukemia</th>\n",
       "      <td>7.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brain</th>\n",
       "      <td>35.7</td>\n",
       "      <td>7.1</td>\n",
       "      <td>28.6</td>\n",
       "      <td>28.6</td>\n",
       "      <td>14.3</td>\n",
       "      <td>14.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breast_2_class</th>\n",
       "      <td>40.0</td>\n",
       "      <td>23.1</td>\n",
       "      <td>16.9</td>\n",
       "      <td>38.5</td>\n",
       "      <td>25.5</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breast_3_class</th>\n",
       "      <td>41.3</td>\n",
       "      <td>32.3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>43.8</td>\n",
       "      <td>31.8</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>20.7 +/- 15.8</td>\n",
       "      <td>12.2 +/- 12.4</td>\n",
       "      <td>8.47 +/- 8.73</td>\n",
       "      <td>20.5 +/- 15.5</td>\n",
       "      <td>13.6 +/- 12.9</td>\n",
       "      <td>6.88 +/- 5.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    SVM - Raw SVM - Feature Selection  \\\n",
       "Dataset Name                                            \n",
       "adenocarcinoma           16.0                    16.0   \n",
       "colon                    14.3                     9.5   \n",
       "prostate                 14.7                     4.4   \n",
       "lymphoma                  0.0                     0.0   \n",
       "nci                      35.0                    30.0   \n",
       "srbct                     2.4                     0.0   \n",
       "leukemia                  7.7                     0.0   \n",
       "brain                    35.7                     7.1   \n",
       "breast_2_class           40.0                    23.1   \n",
       "breast_3_class           41.3                    32.3   \n",
       "Average         20.7 +/- 15.8           12.2 +/- 12.4   \n",
       "\n",
       "               SVM - Performance Increase      KNN - Raw  \\\n",
       "Dataset Name                                               \n",
       "adenocarcinoma                        0.0           16.0   \n",
       "colon                                 4.8           19.0   \n",
       "prostate                             10.3           14.7   \n",
       "lymphoma                              0.0            0.0   \n",
       "nci                                   5.0           35.0   \n",
       "srbct                                 2.4            9.5   \n",
       "leukemia                              7.7            0.0   \n",
       "brain                                28.6           28.6   \n",
       "breast_2_class                       16.9           38.5   \n",
       "breast_3_class                        9.0           43.8   \n",
       "Average                     8.47 +/- 8.73  20.5 +/- 15.5   \n",
       "\n",
       "               KNN - Feature Selection KNN - Performance Increase  \n",
       "Dataset Name                                                       \n",
       "adenocarcinoma                    16.0                        0.0  \n",
       "colon                              9.5                        9.5  \n",
       "prostate                           5.9                        8.8  \n",
       "lymphoma                           0.0                        0.0  \n",
       "nci                               33.3                        1.7  \n",
       "srbct                              0.0                        9.5  \n",
       "leukemia                           0.0                        0.0  \n",
       "brain                             14.3                       14.3  \n",
       "breast_2_class                    25.5                       13.0  \n",
       "breast_3_class                    31.8                       12.0  \n",
       "Average                  13.6 +/- 12.9              6.88 +/- 5.82  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate performance of each feature selection technique for all datasets for SVM and KNN\n",
    "models_to_compare = ['SVM', 'KNN']\n",
    "df_models_to_compare_list = []\n",
    "\n",
    "# loop through each model that we want to compare the performance of the raw data vs the feature selected data\n",
    "for model in models_to_compare:\n",
    "    df_model = df_error_rates.xs(model, level=1, drop_level=True)\n",
    "    df_model[f'{model} - Raw'] = df_model['Raw']\n",
    "    df_model[f'{model} - Feature Selection'] = df_model.min(axis=1)\n",
    "    df_model[f'{model} - Performance Increase'] = df_model[f'{model} - Raw'] - df_model[f'{model} - Feature Selection']\n",
    "    df_model.drop(columns=feature_selection_types, inplace=True)\n",
    "    df_models_to_compare_list.append(df_model)\n",
    "\n",
    "# concat the dataframe for each respective technique and dispay it\n",
    "df_combined_models_to_compare = pd.concat(df_models_to_compare_list, axis=1)\n",
    "average_performance_formatted = [f'{df_combined_models_to_compare[column].mean():0.3} +/- {df_combined_models_to_compare[column].std():0.3}' for column in df_combined_models_to_compare.columns]\n",
    "df_combined_models_to_compare.loc['Average'] = average_performance_formatted\n",
    "df_combined_models_to_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-links",
   "metadata": {},
   "source": [
    "**Table 2:** Performance (% error) of raw data vs feature-selected data with SVM and KNN models for all datasets. Feature selection performance increases across all datasets for SVM ranged from 0% to 28.6% with an average of 8.47% +/- 8.73%, and performance increases across all datasets for KNN ranged from 0% to 14.3% with an average of 6.88% +/- 5.82%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-georgia",
   "metadata": {},
   "source": [
    "#### Compare top performing methods to those from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "killing-somewhere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No Info</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>XGBoost</th>\n",
       "      <th>Neural Network</th>\n",
       "      <th>LDA</th>\n",
       "      <th>KNN</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>SVM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adenocarcinoma</th>\n",
       "      <td>15.8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colon</th>\n",
       "      <td>35.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prostate</th>\n",
       "      <td>49.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>8.8</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lymphoma</th>\n",
       "      <td>32.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nci</th>\n",
       "      <td>85.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>33.3</td>\n",
       "      <td>33.3</td>\n",
       "      <td>19.5</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>srbct</th>\n",
       "      <td>63.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leukemia</th>\n",
       "      <td>28.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brain</th>\n",
       "      <td>76.2</td>\n",
       "      <td>14.3</td>\n",
       "      <td>14.3</td>\n",
       "      <td>21.4</td>\n",
       "      <td>7.1</td>\n",
       "      <td>14.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breast_2_class</th>\n",
       "      <td>42.9</td>\n",
       "      <td>23.1</td>\n",
       "      <td>23.5</td>\n",
       "      <td>23.5</td>\n",
       "      <td>23.1</td>\n",
       "      <td>25.5</td>\n",
       "      <td>24.0</td>\n",
       "      <td>23.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breast_3_class</th>\n",
       "      <td>53.7</td>\n",
       "      <td>28.1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>31.2</td>\n",
       "      <td>32.3</td>\n",
       "      <td>31.8</td>\n",
       "      <td>31.2</td>\n",
       "      <td>32.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>48.3 +/- 21.8</td>\n",
       "      <td>11.4 +/- 10.8</td>\n",
       "      <td>14.7 +/- 12.3</td>\n",
       "      <td>14.0 +/- 14.6</td>\n",
       "      <td>12.0 +/- 13.1</td>\n",
       "      <td>13.6 +/- 12.9</td>\n",
       "      <td>9.46 +/- 11.6</td>\n",
       "      <td>12.2 +/- 12.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      No Info  Random Forest        XGBoost Neural Network  \\\n",
       "Dataset Name                                                                 \n",
       "adenocarcinoma           15.8            8.0            8.0           16.0   \n",
       "colon                    35.5           10.0           14.3            5.0   \n",
       "prostate                 49.0            5.9            8.8            2.9   \n",
       "lymphoma                 32.3            0.0            4.8            0.0   \n",
       "nci                      85.2           25.0           39.0           40.0   \n",
       "srbct                    63.5            0.0            4.8            0.0   \n",
       "leukemia                 28.9            0.0            0.0            0.0   \n",
       "brain                    76.2           14.3           14.3           21.4   \n",
       "breast_2_class           42.9           23.1           23.5           23.5   \n",
       "breast_3_class           53.7           28.1           29.0           31.2   \n",
       "Average         48.3 +/- 21.8  11.4 +/- 10.8  14.7 +/- 12.3  14.0 +/- 14.6   \n",
       "\n",
       "                          LDA            KNN  Logistic Regression  \\\n",
       "Dataset Name                                                        \n",
       "adenocarcinoma           11.5           16.0                 12.0   \n",
       "colon                     9.5            9.5                  5.0   \n",
       "prostate                  2.9            5.9                  2.9   \n",
       "lymphoma                  0.0            0.0                  0.0   \n",
       "nci                      33.3           33.3                 19.5   \n",
       "srbct                     0.0            0.0                  0.0   \n",
       "leukemia                  0.0            0.0                  0.0   \n",
       "brain                     7.1           14.3                  0.0   \n",
       "breast_2_class           23.1           25.5                 24.0   \n",
       "breast_3_class           32.3           31.8                 31.2   \n",
       "Average         12.0 +/- 13.1  13.6 +/- 12.9        9.46 +/- 11.6   \n",
       "\n",
       "                          SVM  \n",
       "Dataset Name                   \n",
       "adenocarcinoma           16.0  \n",
       "colon                     9.5  \n",
       "prostate                  4.4  \n",
       "lymphoma                  0.0  \n",
       "nci                      30.0  \n",
       "srbct                     0.0  \n",
       "leukemia                  0.0  \n",
       "brain                     7.1  \n",
       "breast_2_class           23.1  \n",
       "breast_3_class           32.3  \n",
       "Average         12.2 +/- 12.4  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store best performance dataframes for each classification model for each dataset\n",
    "model_performance_list = []\n",
    "\n",
    "# find best performance for each classification model for each dataset\n",
    "for classification_type in classification_type_list:\n",
    "    df_model = df_error_rates.xs(classification_type, level=1, drop_level=False)\n",
    "    df_model = df_model.droplevel('Classification Type')\n",
    "    df_best_performance = pd.DataFrame(round(df_model.min(axis=1), 3), columns=[classification_type])\n",
    "    model_performance_list.append(df_best_performance)\n",
    "    \n",
    "# concat all best performance dataframes across all datasets\n",
    "df_best_overall_performance = pd.concat(model_performance_list, axis=1)\n",
    "# \"no info\" denotes the minimal error we can make if we use no information from the genes\n",
    "# (i.e., we always bet on the most frequent class)\n",
    "df_best_overall_performance.insert(0, 'No Info', no_info_error_rates_dict.values())\n",
    "df_best_overall_performance['No Info'] = df_best_overall_performance['No Info'] * 100\n",
    "best_performance_average_formatted = [f'{df_best_overall_performance[column].mean():0.3} +/- {df_best_overall_performance[column].std():0.3}' for column in df_best_overall_performance.columns]\n",
    "df_best_overall_performance_with_average = df_best_overall_performance.copy()\n",
    "df_best_overall_performance_with_average.loc['Average'] = best_performance_average_formatted\n",
    "\n",
    "# display best performance dataframes across all datasets for each classification method\n",
    "df_best_overall_performance_with_average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-hurricane",
   "metadata": {},
   "source": [
    "**Table 3:** Performance (% error) of the best feature selection method for each classification type across each dataset. Each model performed substantially better than the `No Info` model in the first column, which is what would be expected from a random guess based on always betting on the most frequent class. Logistic regression narrowly beat Random Forest for the best performing model with an average error of 9.46% +/- 11.6%. However, these values could be skewed by model overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "confidential-amino",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No Info</th>\n",
       "      <th>Our Best Model</th>\n",
       "      <th>Best From Our Models</th>\n",
       "      <th>Best Method From Paper</th>\n",
       "      <th>Best From Paper</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adenocarcinoma</th>\n",
       "      <td>15.8</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colon</th>\n",
       "      <td>35.5</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Srunken Centroids</td>\n",
       "      <td>12.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prostate</th>\n",
       "      <td>49.0</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>2.9</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lymphoma</th>\n",
       "      <td>32.3</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nci</th>\n",
       "      <td>85.2</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>19.5</td>\n",
       "      <td>Nearest Neighbors</td>\n",
       "      <td>23.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>srbct</th>\n",
       "      <td>63.5</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DLDA</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leukemia</th>\n",
       "      <td>28.9</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SVM</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brain</th>\n",
       "      <td>76.2</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SVM</td>\n",
       "      <td>13.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breast_2_class</th>\n",
       "      <td>42.9</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>23.1</td>\n",
       "      <td>Srunken Centroids</td>\n",
       "      <td>32.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breast_3_class</th>\n",
       "      <td>53.7</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>28.1</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>34.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                No Info        Our Best Model  Best From Our Models  \\\n",
       "Dataset Name                                                          \n",
       "adenocarcinoma     15.8         Random Forest                   8.0   \n",
       "colon              35.5        Neural Network                   5.0   \n",
       "prostate           49.0        Neural Network                   2.9   \n",
       "lymphoma           32.3         Random Forest                   0.0   \n",
       "nci                85.2   Logistic Regression                  19.5   \n",
       "srbct              63.5         Random Forest                   0.0   \n",
       "leukemia           28.9         Random Forest                   0.0   \n",
       "brain              76.2   Logistic Regression                   0.0   \n",
       "breast_2_class     42.9         Random Forest                  23.1   \n",
       "breast_3_class     53.7         Random Forest                  28.1   \n",
       "\n",
       "               Best Method From Paper  Best From Paper  \n",
       "Dataset Name                                            \n",
       "adenocarcinoma          Random Forest             12.5  \n",
       "colon               Srunken Centroids             12.2  \n",
       "prostate                Random Forest              6.1  \n",
       "lymphoma                Random Forest              0.9  \n",
       "nci                 Nearest Neighbors             23.7  \n",
       "srbct                            DLDA              1.1  \n",
       "leukemia                          SVM              1.4  \n",
       "brain                             SVM             13.8  \n",
       "breast_2_class      Srunken Centroids             32.4  \n",
       "breast_3_class          Random Forest             34.6  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get best model performance from each of our models\n",
    "best_models = df_best_overall_performance.idxmin(axis=1).to_dict()\n",
    "best_performance = round(df_best_overall_performance.min(axis=1), 3)\n",
    "\n",
    "df_compare_model_performance = pd.DataFrame.from_dict(best_models, orient='index', columns=['Our Best Model'])\n",
    "df_compare_model_performance['Best From Our Models'] = best_performance\n",
    "\n",
    "# compare our best model to the best models from the paper\n",
    "df_compare_model_performance['Best Method From Paper'] = best_classification_method_dict.values()\n",
    "df_compare_model_performance['Best From Paper'] = best_method_error_rates_dict.values()\n",
    "df_compare_model_performance['Best From Paper'] = df_compare_model_performance['Best From Paper'] * 100\n",
    "df_compare_model_performance.insert(0, 'No Info', no_info_error_rates_dict.values())\n",
    "df_compare_model_performance['No Info'] = df_compare_model_performance['No Info'] * 100\n",
    "\n",
    "df_compare_model_performance.index.name = 'Dataset Name'\n",
    "df_compare_model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-paintball",
   "metadata": {},
   "source": [
    "**Table 4:** Performance comparison (% error) of our best methods to those from the paper. Random Forest was most often the best model choice from both the paper and our models. The table also shows that our models were able to perform better than those from the paper, likely due to the additional feature selection techniques used."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbasecondae205318944904618b9ba0f7f5aef68ec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}